{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "m96n9ly6FWL6"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = '/content/homer_bart.zip'\n",
        "\n",
        "\n",
        "extract_dir = '/content/extracted'\n",
        "\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "tEHF4t7E1ezr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU6gm61Ymmw0",
        "outputId": "13a416cf-d9c0-4311-cbf8-5b275ebcaabf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training samples: 243\n",
            "Total test samples: 26\n",
            "Classes: ['Bart', 'Homer']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "img_size = (64, 64)\n",
        "batch_size = 32\n",
        "test_split_ratio = 0.1\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(img_size),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "data_dir = '/content/extracted'\n",
        "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
        "\n",
        "test_size = int(test_split_ratio * len(dataset))\n",
        "train_size = len(dataset) - test_size\n",
        "\n",
        "\n",
        "train_data, test_data = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_set = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "classes = dataset.classes\n",
        "num_classes = len(classes)\n",
        "\n",
        "print(f'Total training samples: {train_size}')\n",
        "print(f'Total test samples: {test_size}')\n",
        "print(f'Classes: {classes}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNchVBcqCM6D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOljwip7_Sho",
        "outputId": "e8909bde-416d-4633-f24b-1b0c1e7177ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ImprovedNN(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=12288, out_features=2048, bias=True)\n",
            "  (bn1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu1): ReLU()\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu2): ReLU()\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu3): ReLU()\n",
            "  (dropout3): Dropout(p=0.5, inplace=False)\n",
            "  (fc4): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu4): ReLU()\n",
            "  (dropout4): Dropout(p=0.5, inplace=False)\n",
            "  (fc5): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (bn5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu5): ReLU()\n",
            "  (dropout5): Dropout(p=0.5, inplace=False)\n",
            "  (fc7): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class ImprovedNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, num_classes):\n",
        "        super(ImprovedNN, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_size3)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc4 = nn.Linear(hidden_size3, hidden_size4)\n",
        "        self.bn4 = nn.BatchNorm1d(hidden_size4)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.dropout4 = nn.Dropout(0.5)\n",
        "\n",
        "\n",
        "        self.fc5 = nn.Linear(hidden_size4, hidden_size5)\n",
        "        self.bn5 = nn.BatchNorm1d(hidden_size5)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.dropout5 = nn.Dropout(0.5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.fc7 = nn.Linear(hidden_size5, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.dropout4(x)\n",
        "\n",
        "        x = self.fc5(x)\n",
        "        x = self.bn5(x)\n",
        "        x = self.relu5(x)\n",
        "        x = self.dropout5(x)\n",
        "\n",
        "        x= self.fc7(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "input_size = 64 * 64 * 3\n",
        "hidden_size1 = 2048\n",
        "hidden_size2 = 1024\n",
        "hidden_size3 = 512\n",
        "hidden_size4 = 256\n",
        "hidden_size5 = 128\n",
        "hidden_size6 = 64\n",
        "\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "\n",
        "model = ImprovedNN(input_size, hidden_size1, hidden_size2, hidden_size3, num_classes)\n",
        "\n",
        "# Print model summary\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bheFRW-Smrk-"
      },
      "outputs": [],
      "source": [
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.01)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSAQEpjtmuiN",
        "outputId": "f302d091-b57f-4786-b33b-89ece0ec0fe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.7534\n",
            "Epoch [2/20], Loss: 0.6879\n",
            "Epoch [3/20], Loss: 0.7069\n",
            "Epoch [4/20], Loss: 0.7181\n",
            "Epoch [5/20], Loss: 0.5813\n",
            "Epoch [6/20], Loss: 0.4771\n",
            "Epoch [7/20], Loss: 0.5075\n",
            "Epoch [8/20], Loss: 0.4242\n",
            "Epoch [9/20], Loss: 0.4627\n",
            "Epoch [10/20], Loss: 0.3867\n",
            "Epoch [11/20], Loss: 0.4040\n",
            "Epoch [12/20], Loss: 0.3680\n",
            "Epoch [13/20], Loss: 0.3822\n",
            "Epoch [14/20], Loss: 0.3430\n",
            "Epoch [15/20], Loss: 0.3336\n",
            "Epoch [16/20], Loss: 0.3756\n",
            "Epoch [17/20], Loss: 0.3723\n",
            "Epoch [18/20], Loss: 0.3987\n",
            "Epoch [19/20], Loss: 0.3389\n",
            "Epoch [20/20], Loss: 0.3809\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.view(inputs.size(0), -1)  # Flatten the input images\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "train_model(model, train_loader, criterion, optimizer, scheduler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thSn4cCDpR6m",
        "outputId": "a39cad12-53b0-4ee8-f552-490390dfcca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.88\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def evaluate_model(model, test_set):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_set:\n",
        "            inputs = inputs.view(inputs.size(0), -1)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}')\n",
        "\n",
        "evaluate_model(model, test_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/homer_bart.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ7q7CFXBFG5",
        "outputId": "7302d151-1cab-4e71-b777-8a398e0105f1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/homer_bart.zip\n",
            "replace Bart/bart58.bmp? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5wg--NV8PaMw"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "source": [
        "seed_value = 1\n",
        "np.random.seed(seed_value)\n",
        "tf.random.set_seed(seed_value)\n",
        "\n",
        "def load_and_resize_images(path, size=(64, 64)):\n",
        "    images = []\n",
        "    for filename in os.listdir(path):\n",
        "        # Load the image\n",
        "        image = cv2.imread(os.path.join(path, filename))\n",
        "        if image is not None:\n",
        "            # Resize the image\n",
        "            image = cv2.resize(image, size)\n",
        "            # Append the image to the list\n",
        "            images.append(image)\n",
        "    return images\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "okJ8Cp6_OuLA"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "homer_image = load_and_resize_images('/content/Homer')\n",
        "bart_image = load_and_resize_images('/content/Bart')\n",
        "\n",
        "all_image = homer_image + bart_image"
      ],
      "metadata": {
        "id": "XUiHDm1wBLVr"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(all_image)\n",
        "y = np.concatenate((np.zeros(len(homer_image)), np.ones(len(bart_image))))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, test_size=0.1, random_state=15)\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k7Gt1yayBZac"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(64, 64, 3)))\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer= opt , loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', test_acc * 100, '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOXC_sjABkz9",
        "outputId": "25bd34a4-c77a-463c-8994-e490b0878b3f"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "8/8 [==============================] - 1s 26ms/step - loss: 0.7186 - accuracy: 0.4711\n",
            "Epoch 2/20\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.6639 - accuracy: 0.6074\n",
            "Epoch 3/20\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.6586 - accuracy: 0.6405\n",
            "Epoch 4/20\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.6452 - accuracy: 0.6570\n",
            "Epoch 5/20\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.6078 - accuracy: 0.6777\n",
            "Epoch 6/20\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.5505 - accuracy: 0.7769\n",
            "Epoch 7/20\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.4946 - accuracy: 0.7893\n",
            "Epoch 8/20\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4661 - accuracy: 0.8017\n",
            "Epoch 9/20\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.5152 - accuracy: 0.7397\n",
            "Epoch 10/20\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4934 - accuracy: 0.7769\n",
            "Epoch 11/20\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.4164 - accuracy: 0.8223\n",
            "Epoch 12/20\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4069 - accuracy: 0.8223\n",
            "Epoch 13/20\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.3767 - accuracy: 0.8223\n",
            "Epoch 14/20\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.3471 - accuracy: 0.8430\n",
            "Epoch 15/20\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.3540 - accuracy: 0.8471\n",
            "Epoch 16/20\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.3844 - accuracy: 0.8306\n",
            "Epoch 17/20\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.3446 - accuracy: 0.8306\n",
            "Epoch 18/20\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3110 - accuracy: 0.8595\n",
            "Epoch 19/20\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2929 - accuracy: 0.8967\n",
            "Epoch 20/20\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2791 - accuracy: 0.9215\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.3536 - accuracy: 0.8889\n",
            "Test accuracy: 88.88888955116272 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}